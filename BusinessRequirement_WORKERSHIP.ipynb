{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from stop_words import get_stop_words\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from gensim import corpora, models\n",
    "import pandas as pd\n",
    "import gensim\n",
    "import pyLDAvis.gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /Users/priya/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "stopwords = stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /Users/priya/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Run in python console\n",
    "import nltk; nltk.download('stopwords')\n",
    "\n",
    "# Run in terminal or command prompt\n",
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Regex\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pprint import pprint\n",
    "\n",
    "# Gensim\n",
    "import gensim\n",
    "import gensim.corpora as corpora\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.models import CoherenceModel\n",
    "\n",
    "# spacy for lemmatization\n",
    "import spacy\n",
    "\n",
    "\n",
    "# Plotting tools\n",
    "import pyLDAvis\n",
    "import pyLDAvis.gensim  # don't skip this\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# Enable logging for gensim - optional\n",
    "import logging\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.ERROR)\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\",category=DeprecationWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Review</th>\n",
       "      <th>Rating</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>nice hotel expensive parking got good deal sta...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ok nothing special charge diamond member hilto...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>nice rooms not 4* experience hotel monaco seat...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>unique, great stay, wonderful time hotel monac...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>great stay great stay, went seahawk game aweso...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              Review  Rating\n",
       "0  nice hotel expensive parking got good deal sta...       4\n",
       "1  ok nothing special charge diamond member hilto...       2\n",
       "2  nice rooms not 4* experience hotel monaco seat...       3\n",
       "3  unique, great stay, wonderful time hotel monac...       5\n",
       "4  great stay great stay, went seahawk game aweso...       5"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('/Users/priya/tripadvisor_hotel_reviews.csv', encoding='utf-8')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "pattern = r'\\b[^\\d\\W]+\\b'\n",
    "tokenizer = RegexpTokenizer(pattern)\n",
    "en_stop = get_stop_words('en')\n",
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NLTK Stop words\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = stopwords.words('english')\n",
    "#stop_words.extend(['from', 'subject', 're', 'edu', 'use'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to list\n",
    "data = df.Review.values.tolist()\n",
    "\n",
    "# Remove Emails\n",
    "#data = [re.sub('\\S*@\\S*\\s?', '', sent) for sent in data]\n",
    "\n",
    "# Remove new line characters\n",
    "data = [re.sub('\\s+', ' ', sent) for sent in data]\n",
    "\n",
    "# Remove distracting single quotes\n",
    "data = [re.sub(\"\\'\", \"\", sent) for sent in data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_accented_chars(text):\n",
    "    \"\"\"remove accented characters from text, e.g. caf√©\"\"\"\n",
    "    text = unidecode.unidecode(text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def expand_contractions(text):\n",
    "    \"\"\"expand shortened words, e.g. don't to do not\"\"\"\n",
    "    text = list(cont.expand_texts([text], precise=True))[0]\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ok', 'nothing', 'special', 'charge', 'diamond', 'member', 'hilton', 'decided', 'chain', 'shot', 'th', 'anniversary', 'seattle', 'start', 'booked', 'suite', 'paid', 'extra', 'website', 'description', 'not', 'suite', 'bedroom', 'bathroom', 'standard', 'hotel', 'room', 'took', 'printed', 'reservation', 'desk', 'showed', 'said', 'things', 'like', 'tv', 'couch', 'ect', 'desk', 'clerk', 'told', 'oh', 'mixed', 'suites', 'description', 'kimpton', 'website', 'sorry', 'free', 'breakfast', 'got', 'kidding', 'embassy', 'suits', 'sitting', 'room', 'bathroom', 'bedroom', 'unlike', 'kimpton', 'calls', 'suite', 'day', 'stay', 'offer', 'correct', 'false', 'advertising', 'send', 'kimpton', 'preferred', 'guest', 'website', 'email', 'asking', 'failure', 'provide', 'suite', 'advertised', 'website', 'reservation', 'description', 'furnished', 'hard', 'copy', 'reservation', 'printout', 'website', 'desk', 'manager', 'duty', 'did', 'not', 'reply', 'solution', 'send', 'email', 'trip', 'guest', 'survey', 'did', 'not', 'follow', 'email', 'mail', 'guess', 'tell', 'concerned', 'guest', 'the', 'staff', 'ranged', 'indifferent', 'not', 'helpful', 'asked', 'desk', 'good', 'breakfast', 'spots', 'neighborhood', 'hood', 'told', 'no', 'hotels', 'gee', 'best', 'breakfast', 'spots', 'seattle', 'block', 'away', 'convenient', 'hotel', 'does', 'not', 'know', 'exist', 'arrived', 'late', 'night', 'pm', 'inside', 'run', 'bellman', 'busy', 'chating', 'cell', 'phone', 'help', 'bags', 'prior', 'arrival', 'emailed', 'hotel', 'inform', 'th', 'anniversary', 'half', 'really', 'picky', 'wanted', 'make', 'sure', 'good', 'got', 'nice', 'email', 'saying', 'like', 'deliver', 'bottle', 'champagne', 'chocolate', 'covered', 'strawberries', 'room', 'arrival', 'celebrate', 'told', 'needed', 'foam', 'pillows', 'arrival', 'no', 'champagne', 'strawberries', 'no', 'foam', 'pillows', 'great', 'room', 'view', 'alley', 'high', 'rise', 'building', 'good', 'not', 'better', 'housekeeping', 'staff', 'cleaner', 'room', 'property', 'impressed', 'left', 'morning', 'shopping', 'room', 'got', 'short', 'trips', 'hours', 'beds', 'comfortable', 'not', 'good', 'ac', 'heat', 'control', 'inch', 'screen', 'bring', 'green', 'shine', 'directly', 'eyes', 'night', 'light', 'sensitive', 'tape', 'controls', 'this', 'not', 'start', 'hotel', 'clean', 'business', 'hotel', 'super', 'high', 'rates', 'better', 'chain', 'hotels', 'seattle']\n"
     ]
    }
   ],
   "source": [
    "def sent_to_words(sentences):\n",
    "    for sentence in sentences:\n",
    "        yield(gensim.utils.simple_preprocess(str(sentence), deacc=True))  # deacc=True removes punctuations\n",
    "\n",
    "data_words = list(sent_to_words(data))\n",
    "\n",
    "print(data_words[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['nice', 'hotel', 'expensive', 'parking', 'got', 'good', 'deal', 'stay', 'hotel', 'anniversary', 'arrived', 'late', 'evening', 'took', 'advice', 'previous', 'reviews', 'did', 'valet_parking', 'check', 'quick', 'easy', 'little', 'disappointed', 'non_existent', 'view', 'room', 'room', 'clean', 'nice', 'size', 'bed', 'comfortable', 'woke', 'stiff', 'neck', 'high', 'pillows', 'not', 'soundproof', 'like', 'heard', 'music', 'room', 'night', 'morning', 'loud', 'bangs', 'doors', 'opening_closing', 'hear', 'people', 'talking', 'hallway', 'maybe', 'just', 'noisy', 'neighbors', 'aveda_bath_products', 'nice', 'did', 'not', 'goldfish', 'stay', 'nice', 'touch', 'taken', 'advantage', 'staying', 'longer', 'location', 'great', 'walking_distance', 'shopping', 'overall', 'nice', 'experience', 'having', 'pay', 'parking', 'night']\n"
     ]
    }
   ],
   "source": [
    "# Build the bigram and trigram models\n",
    "bigram = gensim.models.Phrases(data_words, min_count=5, threshold=100) # higher threshold fewer phrases.\n",
    "trigram = gensim.models.Phrases(bigram[data_words], threshold=100)  \n",
    "\n",
    "# Faster way to get a sentence clubbed as a trigram/bigram [same Output can be received by just using the above code as well]\n",
    "bigram_mod = gensim.models.phrases.Phraser(bigram)\n",
    "trigram_mod = gensim.models.phrases.Phraser(trigram)\n",
    "\n",
    "# See trigram example\n",
    "print(trigram_mod[bigram_mod[data_words[0]]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define functions for stopwords, bigrams, trigrams and lemmatization\n",
    "def remove_stopwords(texts):\n",
    "    return [[word for word in simple_preprocess(str(doc)) if word not in stop_words] for doc in texts]\n",
    "\n",
    "def make_bigrams(texts):\n",
    "    return [bigram_mod[doc] for doc in texts]\n",
    "\n",
    "def make_trigrams(texts):\n",
    "    return [trigram_mod[bigram_mod[doc]] for doc in texts]\n",
    "\n",
    "def lemmatization(texts, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV']):\n",
    "    \"\"\"https://spacy.io/api/annotation\"\"\"\n",
    "    texts_out = []\n",
    "    for sent in texts:\n",
    "        doc = nlp(\" \".join(sent)) \n",
    "        texts_out.append([token.lemma_ for token in doc if token.pos_ in allowed_postags])\n",
    "    return texts_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['nice', 'hotel', 'expensive', 'parking', 'get', 'good', 'deal', 'stay', 'hotel', 'anniversary', 'arrive', 'late', 'evening', 'take', 'advice', 'previous', 'review', 'valet_parke', 'check', 'quick', 'easy', 'little', 'disappointed', 'view', 'room', 'room', 'clean', 'nice', 'size', 'bed', 'comfortable', 'wake', 'stiff', 'neck', 'high', 'pillow', 'soundproof', 'hear', 'music', 'room', 'night', 'morning', 'loud', 'bang', 'door', 'opening_close', 'hear', 'people', 'talk', 'maybe', 'noisy', 'neighbor', 'product', 'nice', 'goldfish', 'stay', 'nice', 'touch', 'take', 'advantage', 'stay', 'long', 'location', 'great', 'walking_distance', 'shopping', 'overall', 'nice', 'experience', 'pay', 'parking', 'night']]\n"
     ]
    }
   ],
   "source": [
    "# Remove Stop Words\n",
    "data_words_nostops = remove_stopwords(data_words)\n",
    "\n",
    "# Form Bigrams\n",
    "data_words_bigrams = make_bigrams(data_words_nostops)\n",
    "\n",
    "# Initialize spacy 'en' model, keeping only tagger component (for efficiency)\n",
    "# python3 -m spacy download en\n",
    "nlp = spacy.load('en', disable=['parser', 'ner'])\n",
    "\n",
    "# Do lemmatization keeping only noun, adj, vb, adv\n",
    "data_lemmatized = lemmatization(data_words_bigrams, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV'])\n",
    "\n",
    "print(data_lemmatized[:1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Dictionary\n",
    "id2word = corpora.Dictionary(data_lemmatized)\n",
    "\n",
    "# Create Corpus\n",
    "texts = data_lemmatized\n",
    "\n",
    "# Term Document Frequency\n",
    "corpus = [id2word.doc2bow(text) for text in texts]\n",
    "\n",
    "# View\n",
    "#print(corpus[:1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run the LDA Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build LDA model\n",
    "lda_model = gensim.models.ldamodel.LdaModel(corpus=corpus,\n",
    "                                           id2word=id2word,\n",
    "                                           num_topics=2, \n",
    "                                           random_state=100,\n",
    "                                           update_every=1,\n",
    "                                           chunksize=200,\n",
    "                                           passes=20,\n",
    "                                           alpha='auto',\n",
    "                                           per_word_topics=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0,\n",
      "  '0.012*\"food\" + 0.012*\"room\" + 0.011*\"time\" + 0.011*\"day\" + 0.011*\"good\" + '\n",
      "  '0.011*\"resort\" + 0.010*\"pool\" + 0.010*\"go\" + 0.009*\"get\" + 0.009*\"people\"'),\n",
      " (1,\n",
      "  '0.049*\"hotel\" + 0.042*\"room\" + 0.026*\"stay\" + 0.017*\"great\" + 0.015*\"good\" '\n",
      "  '+ 0.013*\"staff\" + 0.011*\"night\" + 0.010*\"location\" + 0.010*\"breakfast\" + '\n",
      "  '0.008*\"nice\"')]\n"
     ]
    }
   ],
   "source": [
    "# Print the Keyword in the 10 topics\n",
    "pprint(lda_model.print_topics(num_topics=2, num_words=10))\n",
    "doc_lda = lda_model[corpus]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Print the Dominant Topic of Each Word\n",
    "## The challenge here is that many words might belong to multiple Topics and we need to assign them just one Topic that is of highest priority"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>food</td>\n",
       "      <td>1.215058e-02</td>\n",
       "      <td>1.246198e-03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>room</td>\n",
       "      <td>1.199549e-02</td>\n",
       "      <td>4.227054e-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>time</td>\n",
       "      <td>1.104725e-02</td>\n",
       "      <td>4.244115e-03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>day</td>\n",
       "      <td>1.082985e-02</td>\n",
       "      <td>4.205441e-03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>good</td>\n",
       "      <td>1.067747e-02</td>\n",
       "      <td>1.501901e-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31304</th>\n",
       "      <td>marscarpone</td>\n",
       "      <td>6.265603e-07</td>\n",
       "      <td>5.269287e-07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31305</th>\n",
       "      <td>objectional</td>\n",
       "      <td>6.265593e-07</td>\n",
       "      <td>5.268964e-07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31306</th>\n",
       "      <td>andalucia</td>\n",
       "      <td>6.265593e-07</td>\n",
       "      <td>5.268965e-07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31307</th>\n",
       "      <td>onstreet</td>\n",
       "      <td>6.265591e-07</td>\n",
       "      <td>5.268959e-07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31308</th>\n",
       "      <td>materialize</td>\n",
       "      <td>6.265591e-07</td>\n",
       "      <td>5.268959e-07</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>31309 rows √ó 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              word             0             1\n",
       "0             food  1.215058e-02  1.246198e-03\n",
       "1             room  1.199549e-02  4.227054e-02\n",
       "2             time  1.104725e-02  4.244115e-03\n",
       "3              day  1.082985e-02  4.205441e-03\n",
       "4             good  1.067747e-02  1.501901e-02\n",
       "...            ...           ...           ...\n",
       "31304  marscarpone  6.265603e-07  5.269287e-07\n",
       "31305  objectional  6.265593e-07  5.268964e-07\n",
       "31306    andalucia  6.265593e-07  5.268965e-07\n",
       "31307     onstreet  6.265591e-07  5.268959e-07\n",
       "31308  materialize  6.265591e-07  5.268959e-07\n",
       "\n",
       "[31309 rows x 3 columns]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "counter = 0\n",
    "for i in lda_model.show_topics(formatted = False, num_topics = 2, num_words=len(lda_model.id2word)):\n",
    "    if counter == 0:\n",
    "        topic_num = i[0]\n",
    "        topic_words = i[1]\n",
    "        temp1 = pd.DataFrame(topic_words)\n",
    "        temp1.columns = ['word', topic_num]\n",
    "        counter = counter + 1\n",
    "        #     temp1['topic'] = topic_num\n",
    "    else:\n",
    "        topic_num = i[0]\n",
    "        topic_words = i[1]\n",
    "        temp2 = pd.DataFrame(topic_words)\n",
    "        temp2.columns = ['word', topic_num]\n",
    "        temp1 = pd.merge(temp1, temp2, on = \"word\", how = 'left')\n",
    "temp1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word</th>\n",
       "      <th>main_topic</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>food</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>room</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>time</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>day</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>good</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31304</th>\n",
       "      <td>marscarpone</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31305</th>\n",
       "      <td>objectional</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31306</th>\n",
       "      <td>andalucia</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31307</th>\n",
       "      <td>onstreet</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31308</th>\n",
       "      <td>materialize</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>31309 rows √ó 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              word  main_topic\n",
       "0             food           0\n",
       "1             room           1\n",
       "2             time           0\n",
       "3              day           0\n",
       "4             good           1\n",
       "...            ...         ...\n",
       "31304  marscarpone           0\n",
       "31305  objectional           0\n",
       "31306    andalucia           0\n",
       "31307     onstreet           0\n",
       "31308  materialize           0\n",
       "\n",
       "[31309 rows x 2 columns]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This is the ccode that extarcts the topic that has the highest weight\n",
    "words=temp1\n",
    "main_topic = list()\n",
    "for i in range(len(words)):\n",
    "    top1 = 0\n",
    "    for j in range(1,3):\n",
    "        if words.iloc[i,j] > top1:\n",
    "            top1 = words.iloc[i,j]\n",
    "            top2 = j-1\n",
    "    main_topic.append(top2)\n",
    "#print(i, top2)\n",
    "#print(words.iloc[i, j])\n",
    "words['main_topic'] = main_topic    \n",
    "    \n",
    "    # If all you want is the word and its main topic save it into this dataframe\n",
    "word_main_topic = words[['word', 'main_topic']]\n",
    "word_main_topic\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get Dominant Topic for each Sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import stats\n",
    "import numpy as np\n",
    "\n",
    "dict2={}\n",
    "#split each sentence\n",
    "for review_no in range(len(df)):\n",
    "#for review_no in range(100):\n",
    "    review= df.loc[review_no,'Review']\n",
    "    review=review.replace(',','.')\n",
    "    sentences=review.split('.')\n",
    "\n",
    "#create an empty dictionay\n",
    "    dict1={}\n",
    "#split each word    \n",
    "    for sentence in sentences:\n",
    "        words=sentence.split(' ')\n",
    "        #dominant topic\n",
    "        temp=word_main_topic[word_main_topic['word'].isin(words)]\n",
    "        #use mode to find the max frequency of a topic\n",
    "        array1=np.array(temp['main_topic'].tolist())\n",
    "        m=stats.mode(array1)\n",
    "        if len(m[0])==0:\n",
    "            continue\n",
    "        m=m[0][0]\n",
    "        dict1[sentence]=str(m)\n",
    "        \n",
    "    dict2[review_no]=dict1\n",
    "    \n",
    "import json \n",
    "with open('dominantTopic_sentences1.json','w') as file:\n",
    "    json.dump(dict2,file)   "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
